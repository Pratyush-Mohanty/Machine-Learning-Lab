{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name- Pratyush Mohanty\n",
      "Roll No-118CS0180\n",
      "Date- 2021-10-05\n",
      "Machine learning Lab 6\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "print(\"Name- Pratyush Mohanty\")\n",
    "print(\"Roll No-118CS0180\")\n",
    "print(\"Date- {}\".format(date.today()))\n",
    "print(\"Machine learning Lab 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ID3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GadId3Classifier:\n",
    "    def fit(self, input, output):\n",
    "        data = input.copy()\n",
    "        data[output.name] = output\n",
    "        self.tree = self.decision_tree(data, data, input.columns, output.name)\n",
    "\n",
    "    def predict(self, input):\n",
    "        # convert input data into a dictionary of samples\n",
    "        samples = input.to_dict(orient='records')\n",
    "        predictions = []\n",
    "\n",
    "        # make a prediction for every sample\n",
    "        for sample in samples:\n",
    "            predictions.append(self.make_prediction(sample, self.tree, 1.0))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def entropy(self, attribute_column):\n",
    "        # find unique values and their frequency counts for the given attribute\n",
    "        values, counts = np.unique(attribute_column, return_counts=True)\n",
    "\n",
    "        # calculate entropy for each unique value\n",
    "        entropy_list = []\n",
    "\n",
    "        for i in range(len(values)):\n",
    "            probability = counts[i]/np.sum(counts)\n",
    "            entropy_list.append(-probability*np.log2(probability))\n",
    "\n",
    "        # calculate sum of individual entropy values\n",
    "        total_entropy = np.sum(entropy_list)\n",
    "\n",
    "        return total_entropy\n",
    "\n",
    "    def information_gain(self, data, feature_attribute_name, target_attribute_name):\n",
    "        # find total entropy of given subset\n",
    "        total_entropy = self.entropy(data[target_attribute_name])\n",
    "\n",
    "        # find unique values and their frequency counts for the attribute to be split\n",
    "        values, counts = np.unique(data[feature_attribute_name], return_counts=True)\n",
    "\n",
    "        # calculate weighted entropy of subset\n",
    "        weighted_entropy_list = []\n",
    "\n",
    "        for i in range(len(values)):\n",
    "            subset_probability = counts[i]/np.sum(counts)\n",
    "            subset_entropy = self.entropy(data.where(data[feature_attribute_name]==values[i]).dropna()[target_attribute_name])\n",
    "            weighted_entropy_list.append(subset_probability*subset_entropy)\n",
    "\n",
    "        total_weighted_entropy = np.sum(weighted_entropy_list)\n",
    "\n",
    "        # calculate information gain\n",
    "        information_gain = total_entropy - total_weighted_entropy\n",
    "\n",
    "        return information_gain\n",
    "\n",
    "    def decision_tree(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class=None):\n",
    "        # base cases:\n",
    "        # if data is pure, return the majority class of subset\n",
    "        unique_classes = np.unique(data[target_attribute_name])\n",
    "        if len(unique_classes) <= 1:\n",
    "            return unique_classes[0]\n",
    "        # if subset is empty, ie. no samples, return majority class of original data\n",
    "        elif len(data) == 0:\n",
    "            majority_class_index = np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])\n",
    "            return np.unique(original_data[target_attribute_name])[majority_class_index]\n",
    "        # if data set contains no features to train with, return parent node class\n",
    "        elif len(feature_attribute_names) == 0:\n",
    "            return parent_node_class\n",
    "        # if none of the above are true, construct a branch:\n",
    "        else:\n",
    "          # determine parent node class of current branch\n",
    "            majority_class_index = np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])\n",
    "            parent_node_class = unique_classes[majority_class_index]\n",
    "\n",
    "          # determine information gain values for each feature\n",
    "          # choose feature which best splits the data, ie. highest value\n",
    "            ig_values = [self.information_gain(data, feature, target_attribute_name) for feature in feature_attribute_names]\n",
    "            best_feature_index = np.argmax(ig_values)\n",
    "            best_feature = feature_attribute_names[best_feature_index]\n",
    "\n",
    "          # create tree structure, empty at first\n",
    "            tree = {best_feature: {}}\n",
    "\n",
    "          # remove best feature from available features, it will become the parent node\n",
    "            feature_attribute_names = [i for i in feature_attribute_names if i != best_feature]\n",
    "\n",
    "          # create nodes under parent node\n",
    "            parent_attribute_values = np.unique(data[best_feature])\n",
    "            for value in parent_attribute_values:\n",
    "                sub_data = data.where(data[best_feature] == value).dropna()\n",
    "\n",
    "            # call the algorithm recursively\n",
    "                subtree = self.decision_tree(sub_data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\n",
    "\n",
    "              # add subtree to original tree\n",
    "                tree[best_feature][value] = subtree\n",
    "\n",
    "            return tree\n",
    "\n",
    "    def make_prediction(self, sample, tree, default=1):\n",
    "    # map sample data to tree\n",
    "        for attribute in list(sample.keys()):\n",
    "          # check if feature exists in tree\n",
    "          if attribute in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[attribute][sample[attribute]]\n",
    "            except:\n",
    "                return default\n",
    "\n",
    "            result = tree[attribute][sample[attribute]]\n",
    "\n",
    "            # if more attributes exist within result, recursively find best result\n",
    "            if isinstance(result, dict):\n",
    "                return self.make_prediction(sample, result)\n",
    "            else:\n",
    "                return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = r\"C:\\Users\\Pratyush\\Downloads\\wealth.csv\"\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df = df.replace(\"?\", np.nan)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "age2 = preprocessing.LabelEncoder()\n",
    "age1=age2.fit_transform(df[\"age\"])\n",
    "\n",
    "workclass2 = preprocessing.LabelEncoder()\n",
    "workclass1=workclass2.fit_transform(df[\"workclass\"])\n",
    "\n",
    "fnlwgt2 = preprocessing.LabelEncoder()\n",
    "fnlwgt1=fnlwgt2.fit_transform(df[\"fnlwgt\"])\n",
    "\n",
    "education2 = preprocessing.LabelEncoder()\n",
    "education1=education2.fit_transform(df[\"education\"])\n",
    "\n",
    "education_num2 = preprocessing.LabelEncoder()\n",
    "education_num1=education_num2.fit_transform(df[\"education.num\"])\n",
    "\n",
    "marital_status2 = preprocessing.LabelEncoder()\n",
    "marital_status1=marital_status2.fit_transform(df[\"marital.status\"])\n",
    "\n",
    "occupation2 = preprocessing.LabelEncoder()\n",
    "occupation1=occupation2.fit_transform(df[\"occupation\"])\n",
    "\n",
    "relationship2 = preprocessing.LabelEncoder()\n",
    "relationship1=relationship2.fit_transform(df[\"relationship\"])\n",
    "\n",
    "\n",
    "race2 = preprocessing.LabelEncoder()\n",
    "race1=race2.fit_transform(df[\"race\"])\n",
    "\n",
    "sex2 = preprocessing.LabelEncoder()\n",
    "sex1=sex2.fit_transform(df[\"sex\"])\n",
    "\n",
    "\n",
    "\n",
    "capital_gain2 = preprocessing.LabelEncoder()\n",
    "capital_gain1=capital_gain2.fit_transform(df[\"capital.gain\"])\n",
    "\n",
    "capital_loss2 = preprocessing.LabelEncoder()\n",
    "capital_loss1=capital_loss2.fit_transform(df[\"capital.loss\"])\n",
    "\n",
    "\n",
    "hours_per_week2 = preprocessing.LabelEncoder()\n",
    "hours_per_week1=hours_per_week2.fit_transform(df[\"hours.per.week\"])\n",
    "\n",
    "native_country2 = preprocessing.LabelEncoder()\n",
    "native_country1=native_country2.fit_transform(df[\"native.country\"])\n",
    "\n",
    "income2 = preprocessing.LabelEncoder()\n",
    "income1=income2.fit_transform(df[\"income\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'workclass', 'fnlwgt', 'education', 'education.num',\n",
       "       'marital.status', 'occupation', 'relationship', 'race', 'sex',\n",
       "       'capital.gain', 'capital.loss', 'hours.per.week', 'native.country',\n",
       "       'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>6096</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>6564</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>15770</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>13278</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>7296</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30157</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>17375</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30158</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>15471</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30159</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>7555</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30160</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>7377</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30161</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12060</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30162 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  workclass  fnlwgt  education  education.num  marital.status  \\\n",
       "0       65          2    6096         11              8               6   \n",
       "1       37          2    6564          5              3               0   \n",
       "2       24          2   15770         15              9               5   \n",
       "3       17          2   13278         11              8               0   \n",
       "4       21          2    7296          0              5               5   \n",
       "...    ...        ...     ...        ...            ...             ...   \n",
       "30157    5          2   17375         15              9               4   \n",
       "30158   10          2   15471          7             11               2   \n",
       "30159   23          2    7555         11              8               2   \n",
       "30160   41          2    7377         11              8               6   \n",
       "30161    5          2   12060         11              8               4   \n",
       "\n",
       "       occupation  relationship  race  sex  capital.gain  capital.loss  \\\n",
       "0               3             1     4    0             0            89   \n",
       "1               6             4     4    0             0            88   \n",
       "2               9             3     4    0             0            88   \n",
       "3               7             4     4    0             0            87   \n",
       "4               0             4     4    1             0            87   \n",
       "...           ...           ...   ...  ...           ...           ...   \n",
       "30157          10             1     4    1             0             0   \n",
       "30158          12             5     4    0             0             0   \n",
       "30159           6             0     4    1             0             0   \n",
       "30160           0             4     4    0             0             0   \n",
       "30161           0             3     4    1             0             0   \n",
       "\n",
       "       hours.per.week  native.country  income  \n",
       "0                  17              38       0  \n",
       "1                  39              38       0  \n",
       "2                  39              38       0  \n",
       "3                  44              38       0  \n",
       "4                  39              38       0  \n",
       "...               ...             ...     ...  \n",
       "30157              39              38       0  \n",
       "30158              37              38       0  \n",
       "30159              39              38       1  \n",
       "30160              39              38       0  \n",
       "30161              19              38       0  \n",
       "\n",
       "[30162 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tuples = list(zip(age1, workclass1, fnlwgt1, education1, education_num1,\n",
    "       marital_status1, occupation1, relationship1, race1, sex1,\n",
    "       capital_gain1, capital_loss1, hours_per_week1, native_country1,\n",
    "       income1))\n",
    "   \n",
    "# Assign data to tuples.\n",
    "list_of_tuples \n",
    " \n",
    " \n",
    "# Converting lists of tuples into\n",
    "# pandas Dataframe.\n",
    "df2 = pd.DataFrame(list_of_tuples,\n",
    "                  columns = ['age', 'workclass', 'fnlwgt', 'education', 'education.num',\n",
    "       'marital.status', 'occupation', 'relationship', 'race', 'sex',\n",
    "       'capital.gain', 'capital.loss', 'hours.per.week', 'native.country',\n",
    "       'income'])\n",
    "    \n",
    "# Print data.\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(columns=\"income\")\n",
    "y = df2[\"income\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30162"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit model\n",
    "model = GadId3Classifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5012121212121212"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return accuracy score\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSR,SSE,SST,R**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSR: 401.64\n",
      "SSE: 717.0\n",
      "SST: 1249.73\n",
      "R_2: 0.42627\n"
     ]
    }
   ],
   "source": [
    "mean_y = np.mean(y)\n",
    "mean_y_pred = np.mean(y_pred)\n",
    "\n",
    "\n",
    "ssr=[]\n",
    "for i in y_pred:\n",
    "    ssr.append((i-mean_y)**2)\n",
    "SSR=sum(ssr)\n",
    "\n",
    "#SSR = np.sum((y_pred - mean_y)**2)\n",
    "SSE = np.sum((y[:len(y_pred)] - y_pred)**2)\n",
    "SST = np.sum((y - mean_y)**2)\n",
    "R_2 = 1-(SSE/SST)\n",
    "\n",
    "print(f\"SSR: {round(SSR, 2)}\")\n",
    "print(f\"SSE: {round(SSE, 2)}\")\n",
    "print(f\"SST: {round(SST, 2)}\")\n",
    "print(f\"R_2: {round(R_2, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_test_val > 0.05 so we will not reject the hypothesis\n",
      "t_test value:  1.2573337826130208e-31\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "t_val = (abs(mean_y - mean_y_pred)*((df2.shape[0])**0.5))/np.std(y[:len(y_pred)]-y_pred)\n",
    "\n",
    "t_test_val = (1/(2*math.pi)**0.5)*math.exp(-0.5*t_val)\n",
    "print(\"t_test_val > 0.05 so we will not reject the hypothesis\")\n",
    "print(\"t_test value: \", t_test_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_test_val > 0.05, so we will not reject the hypothesis\n",
      "f_test value:  0.03091116301768488\n"
     ]
    }
   ],
   "source": [
    "f_val = (np.std(y))**2/(np.std(y_pred))**2\n",
    "\n",
    "f_test_val = (1/(2*math.pi)**0.5)*math.exp(-0.5*f_val)\n",
    "print(\"f_test_val > 0.05, so we will not reject the hypothesis\")\n",
    "print(\"f_test value: \", f_test_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GadId3Classifier:\n",
    "    def fit(self, input, output):\n",
    "        data = input.copy()\n",
    "        data[output.name] = output\n",
    "        self.tree = self.decision_tree(data, data, input.columns, output.name)\n",
    "\n",
    "    def predict(self, input):\n",
    "        # convert input data into a dictionary of samples\n",
    "        samples = input.to_dict(orient='records')\n",
    "        predictions = []\n",
    "\n",
    "        # make a prediction for every sample\n",
    "        for sample in samples:\n",
    "            predictions.append(self.make_prediction(sample, self.tree, 1.0))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def entropy(self, attribute_column):\n",
    "        # find unique values and their frequency counts for the given attribute\n",
    "        values, counts = np.unique(attribute_column, return_counts=True)\n",
    "\n",
    "        # calculate entropy for each unique value\n",
    "        entropy_list = []\n",
    "\n",
    "        for i in range(len(values)):\n",
    "            probability = counts[i]/np.sum(counts)\n",
    "            entropy_list.append(-probability*np.log2(probability))\n",
    "\n",
    "        # calculate sum of individual entropy values\n",
    "        total_entropy = np.sum(entropy_list)\n",
    "\n",
    "        return total_entropy\n",
    "\n",
    "    def information_gain(self, data, feature_attribute_name, target_attribute_name):\n",
    "        # find total entropy of given subset\n",
    "        total_entropy = self.entropy(data[target_attribute_name])\n",
    "\n",
    "        # find unique values and their frequency counts for the attribute to be split\n",
    "        values, counts = np.unique(data[feature_attribute_name], return_counts=True)\n",
    "\n",
    "        # calculate weighted entropy of subset\n",
    "        weighted_entropy_list = []\n",
    "\n",
    "        for i in range(len(values)):\n",
    "            subset_probability = counts[i]/np.sum(counts)\n",
    "            subset_entropy = self.entropy(data.where(data[feature_attribute_name]==values[i]).dropna()[target_attribute_name])\n",
    "            weighted_entropy_list.append(subset_probability*subset_entropy)\n",
    "\n",
    "        total_weighted_entropy = np.sum(weighted_entropy_list)\n",
    "\n",
    "        # calculate information gain\n",
    "        information_gain = total_entropy - total_weighted_entropy\n",
    "\n",
    "        return information_gain\n",
    "\n",
    "    def decision_tree(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class=None):\n",
    "        # base cases:\n",
    "        # if data is pure, return the majority class of subset\n",
    "        unique_classes = np.unique(data[target_attribute_name])\n",
    "        if len(unique_classes) <= 1:\n",
    "            return unique_classes[0]\n",
    "        # if subset is empty, ie. no samples, return majority class of original data\n",
    "        elif len(data) == 0:\n",
    "            majority_class_index = np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])\n",
    "            return np.unique(original_data[target_attribute_name])[majority_class_index]\n",
    "        # if data set contains no features to train with, return parent node class\n",
    "        elif len(feature_attribute_names) == 0:\n",
    "            return parent_node_class\n",
    "        # if none of the above are true, construct a branch:\n",
    "        else:\n",
    "          # determine parent node class of current branch\n",
    "            majority_class_index = np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])\n",
    "            parent_node_class = unique_classes[majority_class_index]\n",
    "\n",
    "          # determine information gain values for each feature\n",
    "          # choose feature which best splits the data, ie. highest value\n",
    "            ig_values = [self.information_gain(data, feature, target_attribute_name) for feature in feature_attribute_names]\n",
    "            best_feature_index = np.argmax(ig_values)\n",
    "            best_feature = feature_attribute_names[best_feature_index]\n",
    "\n",
    "          # create tree structure, empty at first\n",
    "            tree = {best_feature: {}}\n",
    "\n",
    "          # remove best feature from available features, it will become the parent node\n",
    "            feature_attribute_names = [i for i in feature_attribute_names if i != best_feature]\n",
    "\n",
    "          # create nodes under parent node\n",
    "            parent_attribute_values = np.unique(data[best_feature])\n",
    "            for value in parent_attribute_values:\n",
    "                sub_data = data.where(data[best_feature] == value).dropna()\n",
    "\n",
    "            # call the algorithm recursively\n",
    "                subtree = self.decision_tree(sub_data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\n",
    "\n",
    "              # add subtree to original tree\n",
    "                tree[best_feature][value] = subtree\n",
    "\n",
    "            return tree\n",
    "\n",
    "    def make_prediction(self, sample, tree, default=1):\n",
    "    # map sample data to tree\n",
    "        for attribute in list(sample.keys()):\n",
    "          # check if feature exists in tree\n",
    "          if attribute in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[attribute][sample[attribute]]\n",
    "            except:\n",
    "                return default\n",
    "\n",
    "            result = tree[attribute][sample[attribute]]\n",
    "\n",
    "            # if more attributes exist within result, recursively find best result\n",
    "            if isinstance(result, dict):\n",
    "                return self.make_prediction(sample, result)\n",
    "            else:\n",
    "                return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = r\"C:\\Users\\Pratyush\\Downloads\\speech.csv\"\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Favorite Color</th>\n",
       "      <th>Favorite Music Genre</th>\n",
       "      <th>Favorite Beverage</th>\n",
       "      <th>Favorite Soft Drink</th>\n",
       "      <th>Gender Probablity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cool</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Vodka</td>\n",
       "      <td>7UP/Sprite</td>\n",
       "      <td>0.782757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>Hip hop</td>\n",
       "      <td>Vodka</td>\n",
       "      <td>Coca Cola/Pepsi</td>\n",
       "      <td>0.692759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Warm</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Wine</td>\n",
       "      <td>Coca Cola/Pepsi</td>\n",
       "      <td>0.841684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Warm</td>\n",
       "      <td>Folk/Traditional</td>\n",
       "      <td>Whiskey</td>\n",
       "      <td>Fanta</td>\n",
       "      <td>0.887022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cool</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Vodka</td>\n",
       "      <td>Coca Cola/Pepsi</td>\n",
       "      <td>0.672850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Favorite Color Favorite Music Genre Favorite Beverage Favorite Soft Drink  \\\n",
       "0           Cool                 Rock             Vodka          7UP/Sprite   \n",
       "1        Neutral              Hip hop             Vodka     Coca Cola/Pepsi   \n",
       "2           Warm                 Rock              Wine     Coca Cola/Pepsi   \n",
       "3           Warm     Folk/Traditional           Whiskey               Fanta   \n",
       "4           Cool                 Rock             Vodka     Coca Cola/Pepsi   \n",
       "\n",
       "   Gender Probablity  \n",
       "0           0.782757  \n",
       "1           0.692759  \n",
       "2           0.841684  \n",
       "3           0.887022  \n",
       "4           0.672850  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Favorite Color', 'Favorite Music Genre', 'Favorite Beverage',\n",
       "       'Favorite Soft Drink', 'Gender Probablity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df = df.replace(\"?\", np.nan)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "Favorite_Color2 = preprocessing.LabelEncoder()\n",
    "Favorite_Color1=Favorite_Color2.fit_transform(df[\"Favorite Color\"])\n",
    "\n",
    "Favorite_Music_Genre2 = preprocessing.LabelEncoder()\n",
    "Favorite_Music_Genre1=Favorite_Music_Genre2.fit_transform(df[\"Favorite Music Genre\"])\n",
    "\n",
    "Favorite_Beverage2 = preprocessing.LabelEncoder()\n",
    "Favorite_Beverage1=Favorite_Beverage2.fit_transform(df[\"Favorite Beverage\"])\n",
    "\n",
    "Favorite_Soft_Drink2 = preprocessing.LabelEncoder()\n",
    "Favorite_Soft_Drink1=Favorite_Soft_Drink2.fit_transform(df[\"Favorite Soft Drink\"])\n",
    "\n",
    "Gender_Probablity2 = preprocessing.LabelEncoder()\n",
    "Gender_Probablity1=Gender_Probablity2.fit_transform(df[\"Gender Probablity\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Favorite Color</th>\n",
       "      <th>Favorite Music Genre</th>\n",
       "      <th>Favorite Beverage</th>\n",
       "      <th>Favorite Soft Drink</th>\n",
       "      <th>Gender Probablity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Favorite Color  Favorite Music Genre  Favorite Beverage  \\\n",
       "0                0                     6                  3   \n",
       "1                1                     2                  3   \n",
       "2                2                     6                  5   \n",
       "3                2                     1                  4   \n",
       "4                0                     6                  3   \n",
       "..             ...                   ...                ...   \n",
       "61               0                     6                  3   \n",
       "62               0                     2                  0   \n",
       "63               1                     2                  1   \n",
       "64               0                     6                  5   \n",
       "65               0                     0                  0   \n",
       "\n",
       "    Favorite Soft Drink  Gender Probablity  \n",
       "0                     0                 55  \n",
       "1                     1                 47  \n",
       "2                     1                 57  \n",
       "3                     2                 61  \n",
       "4                     1                 45  \n",
       "..                  ...                ...  \n",
       "61                    1                  7  \n",
       "62                    1                  3  \n",
       "63                    2                 30  \n",
       "64                    1                 32  \n",
       "65                    1                 18  \n",
       "\n",
       "[66 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tuples = list(zip(Favorite_Color1, Favorite_Music_Genre1, Favorite_Beverage1,\n",
    "       Favorite_Soft_Drink1, Gender_Probablity1))\n",
    "   \n",
    "# Assign data to tuples.\n",
    "list_of_tuples \n",
    " \n",
    " \n",
    "# Converting lists of tuples into\n",
    "# pandas Dataframe.\n",
    "df2 = pd.DataFrame(list_of_tuples,\n",
    "                  columns = ['Favorite Color', 'Favorite Music Genre', 'Favorite Beverage',\n",
    "       'Favorite Soft Drink', 'Gender Probablity'])\n",
    "    \n",
    "# Print data.\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop(columns=\"Gender Probablity\")\n",
    "y = df2[\"Gender Probablity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GadId3Classifier()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "#y_pred=list(map(int,y_pred))\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSR,SSE,SST,R**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSR: 10090.25\n",
      "SSE: 24946.0\n",
      "SST: 23952.5\n",
      "R_2: -0.04148\n"
     ]
    }
   ],
   "source": [
    "mean_y = np.mean(y)\n",
    "mean_y_pred = np.mean(y_pred)\n",
    "\n",
    "\n",
    "ssr=[]\n",
    "for i in y_pred:\n",
    "    ssr.append((i-mean_y)**2)\n",
    "SSR=sum(ssr)\n",
    "\n",
    "#SSR = np.sum((y_pred - mean_y)**2)\n",
    "SSE = np.sum((y[:len(y_pred)] - y_pred)**2)\n",
    "SST = np.sum((y - mean_y)**2)\n",
    "R_2 = 1-(SSE/SST)\n",
    "\n",
    "print(f\"SSR: {round(SSR, 2)}\")\n",
    "print(f\"SSE: {round(SSE, 2)}\")\n",
    "print(f\"SST: {round(SST, 2)}\")\n",
    "print(f\"R_2: {round(R_2, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_test_val > 0.05 so we will not reject the hypothesis\n",
      "t_test value:  0.013821243866011359\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "t_val = (abs(mean_y - mean_y_pred)*((df2.shape[0])**0.5))/np.std(y[:len(y_pred)]-y_pred)\n",
    "\n",
    "t_test_val = (1/(2*math.pi)**0.5)*math.exp(-0.5*t_val)\n",
    "print(\"t_test_val > 0.05 so we will not reject the hypothesis\")\n",
    "print(\"t_test value: \", t_test_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_test_val > 0.05, so we will not reject the hypothesis\n",
      "f_test value:  0.23476637883094345\n"
     ]
    }
   ],
   "source": [
    "f_val = (np.std(y))**2/(np.std(y_pred))**2\n",
    "\n",
    "f_test_val = (1/(2*math.pi)**0.5)*math.exp(-0.5*f_val)\n",
    "print(\"f_test_val > 0.05, so we will not reject the hypothesis\")\n",
    "print(\"f_test value: \", f_test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                          The end\n"
     ]
    }
   ],
   "source": [
    "print(\" \"*58+\"The end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
